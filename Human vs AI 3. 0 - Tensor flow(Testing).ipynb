{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5929d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 19:56:16.406015: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-21 19:56:17.094786: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 19:56:17.095026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 19:56:17.228105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-21 19:56:17.408226: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-21 19:56:17.409432: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-21 19:56:19.367958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5.6414 - accuracy: 0.4286\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 132.0299 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 41.4161 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7099 - accuracy: 0.9286\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.3822 - accuracy: 0.5714\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 6.2920 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.5419 - accuracy: 0.5714\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.1673 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5054 - accuracy: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd90afa20d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fae926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 868ms/step - loss: 1.6606 - accuracy: 0.7143\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 146.1852 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 50.4178 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4.3409 - accuracy: 0.7143\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 23.7489 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.5890 - accuracy: 0.9286\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 9.1819e-16 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.3451e-12 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.8988e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.7346 - accuracy: 0.8571\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "This voice is a Bot voice\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import librosa\n",
    "\n",
    "# Load audio file and extract features\n",
    "def load_and_extract_features(*audio_paths):\n",
    "    mfccs_list = []\n",
    "    pitch_list = []\n",
    "    formants_list = []\n",
    "\n",
    "    for audio_path in audio_paths:\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        pitch = np.mean(librosa.yin(y, fmin=50, fmax=200))\n",
    "        formants = librosa.effects.harmonic(y)\n",
    "\n",
    "        mfccs_list.append(mfccs)\n",
    "        pitch_list.append(pitch)\n",
    "        formants_list.append(formants)\n",
    "\n",
    "    return mfccs_list, pitch_list, formants_list\n",
    "\n",
    "# Define audio paths\n",
    "human_audio_paths = [\n",
    "    'Human-voice1.mp3',\n",
    "    'Human-voice2.mp3',\n",
    "    'Human-voice3.mp3',\n",
    "    'Human-Test-Voice.mp3',\n",
    "    'Human-voice4.mp3',\n",
    "    'Human-voice5.mp3',\n",
    "    'Human-voice6.mp3'\n",
    "]\n",
    "\n",
    "ai_audio_paths = [\n",
    "    'voicemail-354.mp3',\n",
    "    'voicemail-355.mp3',\n",
    "    'voicemail-356.mp3',\n",
    "    'voicemail-357(2).mp3',\n",
    "    'voicemail-357(1).mp3',\n",
    "    'voicemail-358.mp3',\n",
    "    'voicemail-358(1).mp3'\n",
    "]\n",
    "\n",
    "# Extract features for humans and AIs\n",
    "human_mfccs, human_pitch, human_formants = load_and_extract_features(*human_audio_paths)\n",
    "ai_mfccs, ai_pitch, ai_formants = load_and_extract_features(*ai_audio_paths)\n",
    "\n",
    "# Truncate arrays to a fixed length\n",
    "max_length = 500\n",
    "human_mfccs_processed = [arr[:, :max_length] if arr.shape[1] >= max_length else np.pad(arr, ((0, 0), (0, max_length - arr.shape[1])) , mode='constant') for arr in human_mfccs]\n",
    "ai_mfccs_processed = [arr[:, :max_length] if arr.shape[1] >= max_length else np.pad(arr, ((0, 0), (0, max_length - arr.shape[1])) , mode='constant') for arr in ai_mfccs]\n",
    "\n",
    "# Concatenate processed arrays\n",
    "data = np.concatenate((human_mfccs_processed, ai_mfccs_processed), axis=0)\n",
    "\n",
    "# Create labels (0 for human, 1 for AI)\n",
    "labels = np.array([0] * len(human_mfccs_processed) + [1] * len(ai_mfccs_processed))\n",
    "\n",
    "# Reshape data to fit the input shape of the model\n",
    "data = data.reshape((-1, 13, max_length, 1))\n",
    "\n",
    "# Model Definition\n",
    "num_mfcc = data.shape[1]\n",
    "num_frames = data.shape[2]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(num_mfcc, num_frames, 1)),  # Input shape for MFCC features\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Model Training\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Inferencing\n",
    "def classify_audio(audio_path):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_processed = mfccs[:, :max_length] if mfccs.shape[1] >= max_length else np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])) , mode='constant')\n",
    "    features = mfccs_processed.reshape((1, 13, max_length, 1))\n",
    "    predicted_label = model.predict(features)\n",
    "    if predicted_label > 0.5:\n",
    "        return \"This voice is a Bot voice\"\n",
    "    else:\n",
    "        return \"This voice is a Human voice\"\n",
    "\n",
    "#Testing\n",
    "audio_path = 'voicemail-354.mp3'  \n",
    "result = classify_audio(audio_path)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
